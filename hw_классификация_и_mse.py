# -*- coding: utf-8 -*-
"""HW_классификация_и_MSE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bRW-_32-XXNp5F5jT8eTlShdwF-Rd6hX

# Классификация и MSE

Что произойдет если в задаче классификации при обучении применить MSE? вместо возни с log_loss, hinge_loss и прочим
"""

import numpy as np
from sklearn.datasets import make_classification
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

rng = np.random.default_rng(seed=73)
class sgd_lecture_linear:
  def __init__(self, rng, MAX_ITER = 100_000):
    self.MAX_ITER = MAX_ITER
    self.w = None
    self.w0 = rng.normal()
    return

  def _f(self, x):
    assert len(x) == len(self.w)
    return np.dot(self.w, x) + self.w0

  def _der_loss(self, x, y, loss):
    if loss == 'MSE':
      return -(y - self._f(x)) * x

  def fit(self, X_train, y_train, loss = 'MSE'):
    self.w = rng.normal(size=X_train.shape[1])
    step = 0.01
    for k in range(self.MAX_ITER):
      rand_index = rng.integers(0, X_train.shape[0] - 1)
      x = np.array(X_train)[rand_index]
      y = np.array(y_train)[rand_index]
      if k % 10000 == 0:
         step = step / 2
      self.partial_fit(x, y, step, loss)

  def partial_fit(self, x, y, step = 0.01, loss = 'MSE'):
    if not self.w.any():
      self.w = rng.normal(size=len(x))
    dl = self._der_loss(x, y, loss)
    self.w -= step * dl
    self.w0 -= - step * (y - self._f(x))

  def predict_proba(self, x, loss = 'MSE'):
    x = np.array(x)
    preds = []
    pred_fuction = self._f
    for x_curr in x:
      preds.append(pred_fuction(x_curr))
    return preds

random_state = 73
x, y = make_classification(1_000, 7, random_state = random_state)
test_size = 0.3
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=random_state)

sgd = sgd_lecture_linear(rng)
sgd.fit(x_train, y_train)
preds = sgd.predict_proba(x_test)
print(f'{roc_auc_score(y_test, preds):.3f}')

"""Очень неплохо!

Сравним с Logistic Regression из sklearn (обучается на logloss)
"""

lr = LogisticRegression(random_state = random_state)
lr.fit(x_train, y_train)
lr_preds = lr.predict_proba(x_test)[:, 1]
print(f'{roc_auc_score(y_test, lr_preds):.3f}')

"""Не сказать чтобы прямо сильно отличаются качеством!

Как же так? Мы же оптимизировали MSE -- а он для задач регресии!

Попробуем сломать!

Давайте заменим таргеты на -1 и 1 вместо 0 и 1
"""

sgd = sgd_lecture_linear(rng)
sgd.fit(x_train, 2 * y_train - 1)
preds = sgd.predict_proba(x_test)
print(f'{roc_auc_score(2 * y_test - 1, preds):.3f}')

lr = LogisticRegression(random_state = random_state)
lr.fit(x_train, 2 * y_train - 1)
lr_preds = lr.predict_proba(x_test)[:, 1]
print(f'{roc_auc_score(2 * y_test - 1, lr_preds):.3f}')

"""Не сработало!

**Задание**       
А если заменить 0 и 1 на -10 и 10?

1.   Метрики изменятся
2.   Метрики не изменятся
"""

# INSERT YOUR CODE HERE

"""## То есть MSE вполне можно применять для классификации?

В целом да!     
Но мы потеряем некоторые хорошие свойства.     
Вспомним семинар -- оптимизация log_loss эквивалента максимизации правдоподобия, то есть получившиеся оценки это оценки вероятности

Что же нам напредсказывал SGD с MSE?
"""

import pandas as pd
pd.Series(preds).hist()

"""Сравните с предсказаниями логистической регресии"""

pd.Series(lr_preds).hist()

"""То есть если логистическая регрессия предсказала, например, 0.9 -- в 9 случаях из 10 целевое событие произойдет

Это свойство логситической регрессии используется в моделях отклика в прайсинге, в оценке вероятности дефолта в рисках и пр.

Так что же, MSE вообще в классификации не использовать?
"""

from sklearn.calibration import calibration_curve, CalibrationDisplay

"""Для начала посмотрим на калибровочные кривые наших классификаторов

Они строятся так:
1.   Бьем пространство предсказаний на равные бины -- например, точки со скором от 0 до 0.2, от 0.2 до 0.4 и пр
2.   В каждом таком бине считаем средний скор (среднее значение всех предсказание, попавши в бин) -- это горизонтальная координата
3.   В каждом бине считаем долю единичек -- это вертикальная координата
4.   Получившиеся точки соединяем ломаной

При идеальной калибровке средний скор будет совпадать с концентрацией риска

Посмотрим как откалибрована логистическая регрессия
"""

prob_true, prob_pred = calibration_curve(y_test, lr_preds, n_bins=10)
disp = CalibrationDisplay(prob_true, prob_pred, lr_preds)
disp.plot()

"""Почему так получилось?

Смотрим на предыдущую картинку с распределением предиктов -- очень мало наблюдений попало в середину

Теперь как калиброван наш SGD с MSE, предварительно шкалируем предикты
"""

preds = (preds - min(preds)) / max(preds - min(preds))

prob_true, prob_pred = calibration_curve(y_test, preds, n_bins=10)
disp = CalibrationDisplay(prob_true, prob_pred, preds)
disp.plot()

"""Да уж! Вот здесь точно нельзя использовать предикты как вероятность!

Но это на глаз,  а как же количественно сравнить эти две кривые?

Для этого используется специальная метрика**:

$$
\text{Brier Score} = \frac{1}{N} \sum_{i=1}^{N} \left( y_i - p_i \right)^2
$$

** это вариант sklearn, в рисках применяется после разделения на 3 части -- аналогично разделению MSE на bias и variance -- мы еще поговорим об этом в семинаре по ансамблям

**Задание**:

Какой известный лосс вам напомнила эта формула

1.   huber loss
2.   MSE
3.   MSLE
4.   hinge loss
5.   log loss
6.   triplet loss

**Задание**:
В нашем примере как отличается Brier Score у логистической регрессии (LR) и SGD c MSE?

1.   У LR в три и более раз выше
2.   У LR выше от двух до трех раз
3.   Различия несущественны,  составляют не более 10%
4.   У LR ниже от двух до трех раз
5.   У LR в три и более раз ниже
"""

from sklearn.metrics import brier_score_loss, mean_squared_error
from tabulate import tabulate
# INSERT YOUR CODE HERE